{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Events:\n",
      "Event at frame 57, timestamp 2.28s\n",
      "Event at frame 58, timestamp 2.32s\n",
      "Event at frame 59, timestamp 2.36s\n",
      "Event at frame 60, timestamp 2.40s\n",
      "Event at frame 61, timestamp 2.44s\n",
      "Event at frame 62, timestamp 2.48s\n",
      "Event at frame 63, timestamp 2.52s\n",
      "Event at frame 64, timestamp 2.56s\n",
      "Event at frame 65, timestamp 2.60s\n",
      "Event at frame 66, timestamp 2.64s\n",
      "Event at frame 67, timestamp 2.68s\n",
      "Event at frame 68, timestamp 2.72s\n",
      "Event at frame 69, timestamp 2.76s\n",
      "Event at frame 70, timestamp 2.80s\n",
      "Event at frame 71, timestamp 2.84s\n",
      "Event at frame 72, timestamp 2.88s\n",
      "Event at frame 73, timestamp 2.92s\n",
      "Event at frame 74, timestamp 2.96s\n",
      "Event at frame 75, timestamp 3.00s\n",
      "Event at frame 76, timestamp 3.04s\n",
      "Event at frame 77, timestamp 3.08s\n",
      "Event at frame 78, timestamp 3.12s\n",
      "Event at frame 79, timestamp 3.16s\n",
      "Event at frame 80, timestamp 3.20s\n",
      "Event at frame 81, timestamp 3.24s\n",
      "Event at frame 82, timestamp 3.28s\n",
      "Event at frame 83, timestamp 3.32s\n",
      "Event at frame 84, timestamp 3.36s\n",
      "Event at frame 85, timestamp 3.40s\n",
      "Event at frame 86, timestamp 3.44s\n",
      "Event at frame 87, timestamp 3.48s\n",
      "Event at frame 88, timestamp 3.52s\n",
      "Event at frame 89, timestamp 3.56s\n",
      "Event at frame 90, timestamp 3.60s\n",
      "Event at frame 91, timestamp 3.64s\n",
      "Event at frame 92, timestamp 3.68s\n",
      "Event at frame 93, timestamp 3.72s\n",
      "Event at frame 94, timestamp 3.76s\n",
      "Event at frame 95, timestamp 3.80s\n",
      "Event at frame 96, timestamp 3.84s\n",
      "Event at frame 97, timestamp 3.88s\n",
      "Event at frame 98, timestamp 3.92s\n",
      "Event at frame 99, timestamp 3.96s\n",
      "Event at frame 100, timestamp 4.00s\n",
      "Event at frame 102, timestamp 4.08s\n",
      "Event at frame 103, timestamp 4.12s\n",
      "Event at frame 104, timestamp 4.16s\n",
      "Event at frame 105, timestamp 4.20s\n",
      "Event at frame 106, timestamp 4.24s\n",
      "Event at frame 107, timestamp 4.28s\n",
      "Event at frame 108, timestamp 4.32s\n",
      "Event at frame 109, timestamp 4.36s\n",
      "Event at frame 110, timestamp 4.40s\n",
      "Event at frame 111, timestamp 4.44s\n",
      "Event at frame 112, timestamp 4.48s\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "video_path = 'Sample video.mp4' \n",
    "output_path = 'output video.mp4' \n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read the first frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "frame_number = 1\n",
    "event_frames = []\n",
    "motion_threshold = 0.02  \n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    frame_diff = cv2.absdiff(prev_gray, gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, 30, 255, cv2.THRESH_BINARY)\n",
    "    motion_intensity = np.sum(thresh) / (thresh.shape[0] * thresh.shape[1] * 255)\n",
    "\n",
    "    if motion_intensity > motion_threshold:\n",
    "        timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0 \n",
    "        cv2.putText(frame, f\"Event Detected at {timestamp:.2f}s\",\n",
    "                    (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        event_frames.append((frame_number, timestamp))\n",
    "\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(frame, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "    cv2.imshow('Motion Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    prev_gray = gray.copy()\n",
    "    frame_number += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Detected Events:\")\n",
    "for frame_num, timestamp in event_frames:\n",
    "    print(f\"Event at frame {frame_num}, timestamp {timestamp:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Img.jpg\n",
      "  Person 1: Neutral\n",
      "  Person 2: Neutral\n",
      "  Person 3: Neutral\n",
      "  Person 4: Neutral\n",
      "  Person 5: Neutral\n",
      "  Person 6: Happy\n",
      "Overall sentiment: Neutral\n",
      "\n",
      "Annotated image saved to annotatedpeople.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_path = 'Img.jpg' \n",
    "output_image_path = 'annotatedpeople.jpg' \n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "if img is None:\n",
    "    print(f\"Error loading image {image_path}\")\n",
    "else:\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "    skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n",
    "    skin_mask = cv2.dilate(skin_mask, kernel, iterations=2)\n",
    "    skin_mask = cv2.GaussianBlur(skin_mask, (3, 3), 0)\n",
    "\n",
    "    contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    face_regions = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 500:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            aspect_ratio = w / float(h)\n",
    "            if 0.75 < aspect_ratio < 1.3:\n",
    "                face_regions.append((x, y, w, h))\n",
    "\n",
    "    sentiments = []\n",
    "    annotated_img = img.copy()\n",
    "\n",
    "    for (x, y, w, h) in face_regions:\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "\n",
    "        gray_face = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray_face, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        contours_feat, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        features = []\n",
    "        for cnt_feat in contours_feat:\n",
    "            area_feat = cv2.contourArea(cnt_feat)\n",
    "            if area_feat > 50:\n",
    "                fx, fy, fw, fh = cv2.boundingRect(cnt_feat)\n",
    "                features.append((fx, fy, fw, fh))\n",
    "                cv2.rectangle(face_img, (fx, fy), (fx+fw, fy+fh), (255, 0, 0), 1)\n",
    "\n",
    "        face_height = face_img.shape[0]\n",
    "        mouth_candidates = []\n",
    "        for (fx, fy, fw, fh) in features:\n",
    "            if fy > face_height / 2:\n",
    "                mouth_candidates.append((fx, fy, fw, fh))\n",
    "        if mouth_candidates:\n",
    "            mouth = max(mouth_candidates, key=lambda rect: rect[2] * rect[3])\n",
    "            mx, my, mw, mh = mouth\n",
    "            mouth_region = face_img[my:my+mh, mx:mx+mw]\n",
    "            gray_mouth = cv2.cvtColor(mouth_region, cv2.COLOR_BGR2GRAY)\n",
    "            edges = cv2.Canny(gray_mouth, 50, 150)\n",
    "            top_half = edges[0:mh//2, :]\n",
    "            bottom_half = edges[mh//2:mh, :]\n",
    "            top_count = cv2.countNonZero(top_half)\n",
    "            bottom_count = cv2.countNonZero(bottom_half)\n",
    "            if bottom_count > top_count:\n",
    "                sentiment = \"Happy\"\n",
    "            elif top_count > bottom_count:\n",
    "                sentiment = \"Sad\"\n",
    "            else:\n",
    "                sentiment = \"Neutral\"\n",
    "        else:\n",
    "            sentiment = \"Neutral\"\n",
    "\n",
    "        sentiments.append(sentiment)\n",
    "        cv2.rectangle(annotated_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(annotated_img, sentiment, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    print(f\"Image: {image_path}\")\n",
    "    for idx, sentiment in enumerate(sentiments):\n",
    "        print(f\"  Person {idx+1}: {sentiment}\")\n",
    "    if sentiments:\n",
    "        sentiment_counts = {'Happy': 0, 'Sad': 0, 'Neutral': 0}\n",
    "        for sentiment in sentiments:\n",
    "            sentiment_counts[sentiment] += 1\n",
    "        overall_sentiment = max(sentiment_counts, key=sentiment_counts.get)\n",
    "        print(f\"Overall sentiment: {overall_sentiment}\\n\")\n",
    "    else:\n",
    "        print(\"No faces detected.\\n\")\n",
    "\n",
    "    success = cv2.imwrite(output_image_path, annotated_img)\n",
    "    if success:\n",
    "        print(f\"Annotated image saved to {output_image_path}\")\n",
    "    else:\n",
    "        print(f\"Error saving annotated image to {output_image_path}\")\n",
    "\n",
    "    cv2.imshow('Annotated Image', annotated_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Person.jpg\n",
      "Predicted Gender: Female\n",
      "Reason: Narrower jawline relative to eye distance suggests female.\n",
      "Geometric Features: {'eye_distance': 162.0, 'jaw_width': 244.0}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_faces(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_skin = np.array([0, 30, 60], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 150, 255], dtype=np.uint8)\n",
    "    skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    skin_mask = cv2.erode(skin_mask, kernel, iterations=2)\n",
    "    skin_mask = cv2.dilate(skin_mask, kernel, iterations=2)\n",
    "    \n",
    "    contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    face_regions = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 1000:  \n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            aspect_ratio = w / float(h)\n",
    "\n",
    "            if 0.75 < aspect_ratio < 1.3:\n",
    "\n",
    "                if w > 50 and h > 50:\n",
    "                    face_regions.append((x, y, w, h))\n",
    "    return face_regions\n",
    "\n",
    "def extract_geometric_features(landmarks):\n",
    "    features = {}\n",
    "    eye_distance = np.linalg.norm(np.array(landmarks['left_eye']) - np.array(landmarks['right_eye']))\n",
    "\n",
    "    jaw_width = np.linalg.norm(np.array(landmarks['jaw_left']) - np.array(landmarks['jaw_right']))\n",
    "    features['eye_distance'] = eye_distance\n",
    "    features['jaw_width'] = jaw_width\n",
    "    return features\n",
    "\n",
    "def detect_landmarks(face_img):\n",
    "\n",
    "    height, width = face_img.shape[:2]\n",
    "    landmarks = {\n",
    "        'left_eye': (int(width * 0.3), int(height * 0.4)),\n",
    "        'right_eye': (int(width * 0.7), int(height * 0.4)),\n",
    "        'nose_tip': (int(width * 0.5), int(height * 0.6)),\n",
    "        'jaw_left': (int(width * 0.2), int(height * 0.9)),\n",
    "        'jaw_right': (int(width * 0.8), int(height * 0.9)),\n",
    "        'mouth_left': (int(width * 0.4), int(height * 0.75)),\n",
    "        'mouth_right': (int(width * 0.6), int(height * 0.75)),\n",
    "    }\n",
    "    return landmarks\n",
    "\n",
    "def classify_gender(features):\n",
    "    if features['jaw_width'] / features['eye_distance'] > 1.8:\n",
    "        return 'Male', 'Broad jawline relative to eye distance suggests male.'\n",
    "    else:\n",
    "        return 'Female', 'Narrower jawline relative to eye distance suggests female.'\n",
    "\n",
    "def process_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading image {image_path}\")\n",
    "        return\n",
    "\n",
    "    face_regions = detect_faces(img)\n",
    "    annotated_img = img.copy()\n",
    "    if len(face_regions) == 0:\n",
    "        print(f\"No face detected in {image_path}\")\n",
    "        return\n",
    "\n",
    "    for (x, y, w, h) in face_regions:\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        landmarks = detect_landmarks(face_img)\n",
    "        geometric_features = extract_geometric_features(landmarks)\n",
    "        gender, reason = classify_gender(geometric_features)\n",
    "\n",
    "        cv2.rectangle(annotated_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(annotated_img, f\"Predicted: {gender}\", (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        for point in landmarks.values():\n",
    "            point_abs = (point[0] + x, point[1] + y)\n",
    "            cv2.circle(annotated_img, point_abs, 2, (255, 0, 0), -1)\n",
    "\n",
    "        print(f\"Image: {image_path}\")\n",
    "        print(f\"Predicted Gender: {gender}\")\n",
    "        print(f\"Reason: {reason}\")\n",
    "        print(f\"Geometric Features: {geometric_features}\")\n",
    "\n",
    "        cv2.imshow('Pedictor', annotated_img)\n",
    "        cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "image_path = 'Person.jpg' \n",
    "process_image(image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
